{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Hyper Parameters\n",
    "This notebook belongs to a blog post I published on [Medium](https://medium.com/@timoboehm/how-to-optimized-98baec703593). Its purpose is to provide you with a quick and easy example how to apply __Grid Search__, __Randomized Search__, and __Bayesian Optimizaiton__ to hyper parameter optimization problems. I assume a basic familiarity with ```sklearn``` and other common packages. When it comes to Bayesian Optimization, I'll give you a basic idea on how ```hyperopt``` works.\n",
    "\n",
    "<span style=\"color:blue\">__Disclaimer:__ this notebook is about intution! In case of an actual data science project, please refer to the blog post for further links to in-depth material.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I downloaded the dataset from [Kaggle](https://www.kaggle.com/kemical/kickstarter-projects#ks-projects-201801.csv). I restrict the analysis to the following columns from the original dataset:\n",
    "\n",
    "- ```main_category```: as the name suggest, this indicates the larger category of a kickstarter project.\n",
    "- ```category```: this is the more specific category. For instance, ```main_category``` could be _\"Film\"_ and ```category``` could be _\"Shorts\"_ \n",
    "- ```country```: exactly what the name suggests. Although most projects are based in the US (about 77%), there is also some variation that might help us to predict the result.\n",
    "- ```usd_goal_real```: this feature already normalized the funding goal to USD.\n",
    "- ```state```: this is the target variable we are looking for. For this notebook I only consider _\"succesful\"_ vs _\"failed\"_ and _\"canceled\"_. Observations with _\"live\"_ or _\"undefined\" are excluded from the analysis.\n",
    "\n",
    "I sample 1000 observations to accelerate the calculation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>main_category</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>usd_goal_real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>316713</th>\n",
       "      <td>Restaurants</td>\n",
       "      <td>Food</td>\n",
       "      <td>failed</td>\n",
       "      <td>US</td>\n",
       "      <td>25000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275116</th>\n",
       "      <td>Product Design</td>\n",
       "      <td>Design</td>\n",
       "      <td>failed</td>\n",
       "      <td>NL</td>\n",
       "      <td>40180.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360753</th>\n",
       "      <td>Food</td>\n",
       "      <td>Food</td>\n",
       "      <td>failed</td>\n",
       "      <td>GB</td>\n",
       "      <td>37097.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74036</th>\n",
       "      <td>Fiction</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>canceled</td>\n",
       "      <td>US</td>\n",
       "      <td>6700.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140396</th>\n",
       "      <td>Farms</td>\n",
       "      <td>Food</td>\n",
       "      <td>failed</td>\n",
       "      <td>US</td>\n",
       "      <td>35000.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              category main_category     state country  usd_goal_real\n",
       "316713     Restaurants          Food    failed      US       25000.00\n",
       "275116  Product Design        Design    failed      NL       40180.32\n",
       "360753            Food          Food    failed      GB       37097.62\n",
       "74036          Fiction    Publishing  canceled      US        6700.00\n",
       "140396           Farms          Food    failed      US       35000.00"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv(\"data/kickstarter_projects.csv\", usecols=[\"category\", \"main_category\", \"state\",\n",
    "                                                             \"country\", \"usd_goal_real\"])\n",
    "\n",
    "# Filter based on the \"state\" column and sample 1000 observations from the remaining set.\n",
    "data = data.loc[data.state.isin([\"successful\", \"failed\", \"canceled\"])].sample(1000, random_state=123)\n",
    "\n",
    "# Look at five random examples from the dataset to understand the basic data structure.\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: run this cell if you want to have a closer look at the five most frequent values for all categorical features.\n",
    "\n",
    "#for c in data.columns:\n",
    "#    if data[c].dtype == \"object\":\n",
    "#        print(f\"----------- {c} -----------\\n\")\n",
    "#        print(f\"{data[c].value_counts(normalize=True)[:5]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in the training set:    800\n",
      "Number of observations in the testing set:     200\n",
      "\n",
      "Number of features available to predict outcome: 165\n",
      "Proportion of successful kickstarter projects: 0.35\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(\"state\", axis=1)\n",
    "X = pd.get_dummies(X)\n",
    "y = data.state == \"successful\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, train_size=.8, random_state=42)\n",
    "\n",
    "print(f\"Number of observations in the training set: {X_train.shape[0]:{6}}\")\n",
    "print(f\"Number of observations in the testing set:  {X_test.shape[0]:{6}}\")\n",
    "print(f\"\\nNumber of features available to predict outcome: {X_train.shape[1]}\")\n",
    "print(f\"Proportion of successful kickstarter projects: {np.mean(y):.{2}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Baseline Classifier to refer to in the next steps\n",
    "clf = GradientBoostingClassifier(random_state=27)\n",
    "\n",
    "# Dictionary to change the two hyper parameters we are focusing on here.\n",
    "hyper_params = {\n",
    "    \"learning_rate\": [.1],\n",
    "    \"max_features\": [None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to evaluate the final models from each approach\n",
    "def eval_final_model(classifier, x_test, y_test):\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    return roc_auc_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these values to decide on the range of the parameters and with how many possibilities you want to start your grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of runs for GridSearch will be:            25\n",
      "Number of runs for RandomizedSearch will be:      12\n",
      "Number of runs for Bayesian Optimization will be: 25\n"
     ]
    }
   ],
   "source": [
    "learning_rate_min, learning_rate_max = .005, .2\n",
    "max_features_min, max_features_max = 1, X_train.shape[1]\n",
    "\n",
    "no_parameters = 5\n",
    "\n",
    "print(f\"Number of runs for GridSearch will be:            {no_parameters**2}\")\n",
    "print(f\"Number of runs for RandomizedSearch will be:      {int(no_parameters**2/2)}\")\n",
    "print(f\"Number of runs for Bayesian Optimization will be: {no_parameters**2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Grid Search\n",
    "The grid is constructed with ```no_parameters``` values for each hyper parameter with linear distances between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate\n",
      "[ 0.005    0.05375  0.1025   0.15125  0.2    ]\n",
      "max_features\n",
      "[1, 42, 83, 124, 165]\n"
     ]
    }
   ],
   "source": [
    "hyper_params[\"learning_rate\"] = np.linspace(learning_rate_min, learning_rate_max, no_parameters)\n",
    "hyper_params[\"max_features\"] = [int(n) for n in np.linspace(max_features_min, max_features_max, no_parameters)]\n",
    "\n",
    "for k,v in hyper_params.items():\n",
    "    print(f\"{k}\\n{v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this set of hyper parameters, we can now run sklearn's ```GridSearchCV``` implementation. I only defined the parameteres that are absolutely necessary. For more information, go [here](MISSING_LINK) <span style=\"color:red\">ADD LINK </span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the best model: 0.626\n",
      "Wall time: 7.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid = GridSearchCV(clf,\n",
    "                    hyper_params,\n",
    "                    verbose=0)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Score of the best model: {eval_final_model(grid.best_estimator_, X_test, y_test):.{3}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying grid search to our hyper parameter optimization problem, it took <span style=\"color:blue\">about 7 seconds</span> for a score of <span style=\"color:blue\">0.626</span> on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Randomized Search\n",
    "We can now take advantage of defining a distribution of possible values instead of hard-coded options. I use scipy's ```randint``` function, that creates random integers within defined boundaries. The options for learning rates stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate\n",
      "[ 0.005    0.05375  0.1025   0.15125  0.2    ]\n",
      "max_features\n",
      "<scipy.stats._distn_infrastructure.rv_frozen object at 0x00000260AE3AAF60>\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint as randint\n",
    "\n",
    "hyper_params[\"max_features\"] = randint(max_features_min, max_features_max)\n",
    "for k,v in hyper_params.items():\n",
    "    print(f\"{k}\\n{v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we use sklearn but this time ```RandomizedSearchCV``` to implement randomized search. Remember that we use randomized search to save time, so we define the number of iterations (```n_iter```) as half of the original grid's size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the best model: 0.617\n",
      "Wall time: 4.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "random = RandomizedSearchCV(clf,\n",
    "                            hyper_params,\n",
    "                            verbose=0,\n",
    "                            n_iter=int(no_parameters**2 / 2))\n",
    "random.fit(X_train, y_train)\n",
    "print(f\"Score of the best model: {eval_final_model(random.best_estimator_, X_test, y_test):.{3}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, randomized search takes only about half the time than grid search (<span style=\"color:blue\">about 3.5 seconds</span>) but its performance went down to <span style=\"color:blue\">0.617</span>. That seems to be acceptable given that we spend only half the time with computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Option 3: Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a extensive example how to implenent ```hyperopt``` go [here](https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a). Bayesian optimization is conceptually harder than grid or randomized search and the necessary code reflects that. To get the intuition, think about the following steps:\n",
    "\n",
    "- As in your everyday optimization problem, you have to define something to minimize. We used ```roc_auc_score```up to now, so we need to implement the _distance to a perfect score_ instead of the raw score. That's what ```min_score``` does.\n",
    "- We also have to define our search space for hyper parameters. This time we use hyperopt's functions, but the idea stays the same.\n",
    "- The other two parameters concern the actual algorithm to do the optimization and the number of allowed evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp   # Used to define the distribution for both parameters\n",
    "from hyperopt import tpe  # Algorithm to apply\n",
    "from hyperopt import fmin # We want to minimze the score_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def min_score(params):\n",
    "    params[\"max_features\"] = int(params[\"max_features\"])\n",
    "    clf = GradientBoostingClassifier(**params)\n",
    "    loss = np.mean(1 - cross_val_score(clf, X_train, y_train, scoring=\"roc_auc\")) # We need to return something to minimize\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'max_features': hp.quniform('max_features', max_features_min, max_features_max, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(learning_rate_min), np.log(learning_rate_max))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of the best model: 0.635\n",
      "Wall time: 6.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best = fmin(fn = min_score, space = hyper_params, algo = tpe.suggest, max_evals = no_parameters**2)\n",
    "best[\"max_features\"] = int(best[\"max_features\"])\n",
    "clf = GradientBoostingClassifier(**best)\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Score of the best model: {eval_final_model(clf, X_test, y_test):.{3}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are back up to <span style=\"color:blue\">about 6.5 seconds</span> but the performance rose to <span style=\"color:blue\">0.635</span>. Given these results, there are two main take-aways:\n",
    "1. Use randomized search instead of grid search.\n",
    "2. Bayesian optimization is a powerful approach with a promising balance between calculating time and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
